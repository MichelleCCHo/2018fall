{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM Lab\n",
    "\n",
    "##### Keywords: maximum likelihood, mixture model,  full-data likelihood, x-likelihood, latent variables, log-likelihood, training set, normal distribution, z-posterior\n",
    "##### Data: ncog.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michelleho/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\isum}{\\sum_{i}}$$\n",
    "$$\\newcommand{\\zsum}{\\sum_{k=1}^{K}}$$\n",
    "$$\\newcommand{\\zsumi}{\\sum_{\\{z_i\\}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EM Algorithm\n",
    "\n",
    "### E-step\n",
    "\n",
    "These observations set up the EM algorithm for us. If we choose, in the **E-step**, at some (possibly initial) value of the parameters $\\theta_{old}$,\n",
    "\n",
    "$$q(z) = p(z  \\vert  x, \\theta_{old}),$$ \n",
    "\n",
    "we then set the Kullback Liebler divergence to 0, and thus $\\mathcal{L}(q, \\theta)$ to the log-likelihood at $\\theta_{old}$,  and maximizing the lower bound. \n",
    "\n",
    "Using this missing data posterior, conditioned on observed data, and $\\theta_{old}$, we compute the expectation of the missing data with respect to the posterior and use it later.\n",
    "\n",
    "![](images/klsplitestep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the **M-step**. \n",
    "\n",
    "Since after the E-step, the lower bound touches the log-likelihood, any maximization of this ELBO from its current value with respect to $\\theta$ will also “push up” on the likelihood itself. Thus M step guaranteedly modifies the parameters $\\theta$ to increase (or keep same) the likelihood of the observed data.\n",
    "\n",
    "Thus we hold now the distribution $q(z)$ fixed at the hidden variable posterior calculated at $\\theta_{old}$, and maximize $\\mathcal{L}(q, \\theta)$ with respect to $\\theta$ to obtain new parameter values $\\theta_{new}$. This is a regular maximization.\n",
    "\n",
    "The distribution $q$, calculated as it is at $\\theta_{old}$ will not in general equal the new posterior distribution $p(z \\vert x,\\theta_{new})$, and hence there will be a nonzero KL divergence. Thus the increase in the log-likelihood will be greater than the increase in the lower bound $\\mathcal{L}$, as illustrated below.\n",
    "\n",
    "The M in “M-step” and “EM” stands for “maximization”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/klsplitmstep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process\n",
    "\n",
    "Note that since $\\mathcal{L}$ is maximized with respect to $\\theta$, one can equivalently maximize the expectation of the full-data log likelihood $\\mathrm{E_q[\\ell( x,z  \\vert  \\theta)]}$ in the M-step since the difference is purely a function of $q$. Furthermore, if the joint distribution $p(x, z \\vert  \\theta)$ is a member of the exponential family, the log-likelihood will have a particularly simple form and will lead to a much simpler maximization than that of the incomple-data log-likelihood $p(x \\vert \\theta)$.\n",
    "\n",
    "We now set $\\theta_{old} = \\theta_{new}$ and repeat the process. This **EM algorithm** is presented and  illustrated below:\n",
    "\n",
    "![](images/emupdate.png)\n",
    "\n",
    "1. We start with the log-likelihood $p(x  \\vert  \\theta)$(red curve) and the initial guess $\\theta_{old}$ of the parameter values\n",
    "2. Until convergence (the $\\theta$ values dont change too much):\n",
    "    1. E-step: Evaluate the hidden variable posterior $q(z, \\theta_{old}) = p(z  \\vert  x, \\theta_{old})$ which gives rise to a lower bound function of $\\theta$: $\\mathcal{L}(q(z, \\theta_{old}), \\theta)$(blue curve) whose value equals the value of $p(x  \\vert  \\theta)$ at $\\theta_{old}$.\n",
    "    2. M-step: maximize the lower bound function with respect to $\\theta$ to get $\\theta_{new}$.\n",
    "    3. Set $\\theta_{old} = \\theta_{new}$\n",
    "    \n",
    "One iteration more is illustrated above, where the subsequent E-step constructs a new lower-bound function that is tangential to the log-likelihood at $\\theta_{new}$, and whose value at $\\theta_{new}$ is higher than the lower bound at $\\theta_{old}$ from the previous step.\n",
    "\n",
    "Thus\n",
    "\n",
    "$$\\ell(\\theta_{t+1}) \\ge \\mathcal{L}(q(z,\\theta_t), \\theta_{t+1}) \\ge \\mathcal{L}(q(z,\\theta_t), \\theta_{t}) = \\ell(\\theta_t)$$\n",
    "\n",
    "The first equality follows since $\\mathcal{L}$ is a lower bound on $\\ell$, the second from the M-step's maximization of $\\mathcal{L}$, and the last from the vanishing of the KL-divergence after the E-step. As a consequence, you **must** observe monotonic increase of the observed-data log likelihood $\\ell$ across iterations. **This is a  powerful debugging tool for your code**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Mixture model using EM\n",
    "\n",
    "We dont know how to solve for the MLE of the unsupervised problem. The EM algorithm comes to the rescue. As described above here is the algorithm:\n",
    "\n",
    "\n",
    "* Repeat until convergence \n",
    "*  E-step: For each $i,j$ calculate \n",
    "\n",
    "$$ w_{i,j} = q_i(z_i=j)=p(z_i=j \\vert  x_i, \\lambda, \\mu, \\Sigma) $$\n",
    "     \n",
    "* M-step: We need to maximize, with respect to our parameters the\n",
    "  \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\mathcal{L} &=& \\sum_i \\sum_{z_i} q_i(z_i) \\log \\frac{p(x_i,z_i  \\vert \\lambda, \\mu, \\Sigma)}{q_i(z_i)} \\nonumber \\\\\n",
    " \\mathcal{L} &=& \\sum_i \\sum_{j=i}^{k}  q_i(z_i=j) \\log \\frac{p(x_i \\vert z_i=j , \\mu, \\Sigma) p(z_i=j \\vert \\lambda)}{q_i(z_i=j)} \\\\\n",
    " \\mathcal{L} & =&  \\sum_{i=1}^{m} \\sum_{j=i}^{k} w_{i,j}  \\log \\left[   \\frac{ \\frac{1}{ (2\\pi)^{n/2} \\vert \\Sigma_j \\vert ^{1/2}} \\exp \\left(    -\\frac{1}{2}(x_i-\\mu_j)^T \\Sigma_j^{-1} (x_i-\\mu_j) \\right)  \\, \\lambda_j   }{w_{i,j}}\\right]\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Taking the derivatives yields the following updating formulas:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\lambda_j &=& \\frac{1}{m} \\sum_{i=1}^m w_{i,j} \\nonumber \\\\ \n",
    " \\mu_j&=& \\frac{ \\sum_{i=1}^m  w_{i,j} \\, x_i}{ \\sum_{i=1}^m  w_{i,j}} \\nonumber \\\\ \n",
    " \\Sigma_j &=& \\frac{ \\sum_{i=1}^m  w_{i,j} \\, (x_i-\\mu_j)(x_i-\\mu_j)^T}{ \\sum_{i=1}^m  w_{i,j}}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "To calculate the E-step we basically calculating the posterior of the  $z$'s given the $x$'s and the\n",
    "current estimate of our parameters. We can use Bayes rule \n",
    "\n",
    "$$ w_{i,j}= p(z_i=j \\vert  x_i, \\lambda, \\mu, \\Sigma) = \\frac{p( x_i \\vert  z_i=j,  \\mu, \\Sigma)\\, p(z_i=j \\vert \\lambda)}{\\sum_{l=1}^k p(x_i  \\vert  z_i=l,  \\mu, \\Sigma) \\, p(z_i=l \\vert \\lambda)} $$\n",
    "\n",
    "Where $p(x_i  \\vert  z_i =j,  \\mu, \\Sigma)$ is the density of the Gaussian with mean $\\mu_j$ and covariance \n",
    "$\\Sigma_j$ at $x_i$ and $p(z_i=j \\vert  \\lambda)$ is simply $\\lambda_j$. \n",
    "If we to compare these formulas in the M-step with the ones we found in GDA we can see\n",
    "that are very similar except that instead of using $\\delta$ functions we use the $w$'s. Thus the EM algorithm corresponds here to a weighted maximum likelihood and the weights are interpreted as the 'probability' of coming from that Gaussian instead of the deterministic \n",
    "$\\delta$ functions. Thus we have achived a **soft clustering** (as opposed to k-means in the unsupervised case and classification in the supervised case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2a75b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#In 1-D\n",
    "# True parameter values\n",
    "mu_true = [2, 5]\n",
    "sigma_true = [0.6, 0.6]\n",
    "lambda_true = .4\n",
    "n = 1000\n",
    "\n",
    "# Simulate from each distribution according to mixing proportion psi\n",
    "z = np.random.binomial(1, lambda_true, n)\n",
    "x = np.array([np.random.normal(mu_true[i], sigma_true[i]) for i in z])\n",
    "\n",
    "plt.hist(x, bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The 1000 z's we used in our simulation but which we shall promptly forget\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Bios366 lecture notes\n",
    "from scipy.stats.distributions import norm\n",
    "\n",
    "def Estep(x, mu, sigma, lam):\n",
    "    a = lam * norm.pdf(x, mu[0], sigma[0])\n",
    "    b = (1. - lam) * norm.pdf(x, mu[1], sigma[1])\n",
    "    return b / (a + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mstep(x, w):\n",
    "    lam = np.mean(1.-w) \n",
    "    \n",
    "    mu = [np.sum((1-w) * x)/np.sum(1-w), np.sum(w * x)/np.sum(w)]\n",
    "    \n",
    "    sigma = [np.sqrt(np.sum((1-w) * (x - mu[0])**2)/np.sum(1-w)), \n",
    "             np.sqrt(np.sum(w * (x - mu[1])**2)/np.sum(w))]\n",
    "    \n",
    "    return mu, sigma, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 [2, 5] [0.6, 0.6]\n",
      "Initials, mu: [-3.27370538 13.18149367]\n",
      "Initials, sigma: [0.67264541 1.93315846]\n",
      "Initials, lam: 0.3277268449314211\n",
      "Iterations 62\n",
      "A: N(2.0257, 0.5942)\n",
      "B: N(5.0125, 0.6058)\n",
      "lam: 0.5888\n"
     ]
    }
   ],
   "source": [
    "print(lambda_true, mu_true, sigma_true)\n",
    "# Initialize values\n",
    "mu = np.random.normal(4, 10, size=2)\n",
    "sigma = np.random.uniform(0, 5, size=2)\n",
    "lam = np.random.random()\n",
    "print(\"Initials, mu:\", mu)\n",
    "print(\"Initials, sigma:\", sigma)\n",
    "print(\"Initials, lam:\", lam)\n",
    "\n",
    "# Stopping criterion\n",
    "crit = 1e-15\n",
    "\n",
    "# Convergence flag\n",
    "converged = False\n",
    "\n",
    "# Loop until converged\n",
    "iterations=1\n",
    "\n",
    "\n",
    "while not converged:\n",
    "    # E-step\n",
    "    if np.isnan(mu[0]) or np.isnan(mu[1]) or np.isnan(sigma[0]) or np.isnan(sigma[1]):\n",
    "        print(\"Singularity!\")\n",
    "        break\n",
    "        \n",
    "    w = Estep(x, mu, sigma, lam)\n",
    "\n",
    "    # M-step\n",
    "    mu_new, sigma_new, lam_new = Mstep(x, w)\n",
    "    \n",
    "    # Check convergence\n",
    "    converged = ((np.abs(lam_new - lam) < crit) \n",
    "                 & np.all(np.abs((np.array(mu_new) - np.array(mu)) < crit))\n",
    "                 & np.all(np.abs((np.array(sigma_new) - np.array(sigma)) < crit)))\n",
    "    mu, sigma, lam = mu_new, sigma_new, lam_new\n",
    "    iterations +=1           \n",
    "\n",
    "print(\"Iterations\", iterations)\n",
    "print('A: N({0:.4f}, {1:.4f})\\nB: N({2:.4f}, {3:.4f})\\nlam: {4:.4f}'.format(\n",
    "                        mu_new[0], sigma_new[0], mu_new[1], sigma_new[1], lam_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99787856e-01, 2.45999765e-08, 9.99992371e-01, 1.08672116e-04,\n",
       "       5.16395375e-08, 2.42485093e-07, 1.36761215e-02, 9.99999905e-01,\n",
       "       1.05554767e-06, 4.61017423e-04, 6.84866370e-07, 3.50211327e-07,\n",
       "       7.75852866e-07, 2.21922121e-07, 1.50610608e-04, 9.99999981e-01,\n",
       "       9.98912460e-01, 4.36444493e-02, 3.49047627e-06, 9.99999770e-01,\n",
       "       9.99999994e-01, 9.99999970e-01, 9.99991264e-01, 9.99991532e-01,\n",
       "       9.99999995e-01, 2.21269155e-11, 3.46991788e-10, 1.25959348e-07,\n",
       "       4.64883588e-01, 9.99999987e-01, 9.99997938e-01, 1.00000000e+00,\n",
       "       1.32990437e-06, 9.99997819e-01, 9.99960035e-01, 1.00000000e+00,\n",
       "       9.99999837e-01, 9.99967182e-01, 2.07285973e-06, 9.99998179e-01,\n",
       "       9.99999506e-01, 9.91964431e-01, 3.34415726e-06, 6.77505973e-08,\n",
       "       9.98312009e-01, 9.99992062e-01, 8.55310914e-07, 9.99999950e-01,\n",
       "       9.99997991e-01, 4.69427519e-07, 7.70047999e-04, 9.99999998e-01,\n",
       "       9.99999748e-01, 4.30985280e-05, 1.34354637e-04, 9.64900772e-01,\n",
       "       9.99999999e-01, 9.99883355e-01, 1.06966918e-05, 9.99990789e-01,\n",
       "       1.34655929e-06, 1.00000000e+00, 9.99999945e-01, 9.64578876e-08,\n",
       "       6.52423908e-01, 9.96802945e-01, 9.94854998e-01, 9.99999729e-01,\n",
       "       7.92568970e-05, 9.99999899e-01, 1.19276485e-05, 4.20054607e-08,\n",
       "       9.79599650e-01, 9.99999979e-01, 6.48933992e-03, 7.57344301e-08,\n",
       "       9.99229244e-01, 2.13234913e-06, 9.99973662e-01, 1.65182102e-04,\n",
       "       2.50621629e-06, 9.99999915e-01, 9.99999984e-01, 1.25634480e-07,\n",
       "       9.99998702e-01, 9.99999441e-01, 9.35373051e-01, 1.10487287e-04,\n",
       "       2.78850028e-06, 7.53006243e-05, 9.98903925e-01, 8.75358530e-08,\n",
       "       8.88358622e-08, 4.07829866e-05, 1.61938154e-05, 4.72937633e-04,\n",
       "       9.99997726e-01, 9.99991066e-01, 9.99989185e-01, 9.99971270e-01,\n",
       "       1.94439411e-08, 3.40460391e-09, 9.99984074e-01, 8.49365946e-01,\n",
       "       1.31108255e-07, 4.93368699e-06, 9.99999479e-01, 1.45602941e-05,\n",
       "       9.99965158e-01, 6.58948689e-08, 9.38709832e-04, 1.08525124e-07,\n",
       "       9.99993704e-01, 1.23246104e-07, 9.99999991e-01, 1.06329580e-04,\n",
       "       3.48382039e-06, 1.80908134e-05, 9.98174589e-01, 9.99997434e-01,\n",
       "       1.44339601e-07, 5.57086967e-05, 1.54967048e-08, 2.75208638e-06,\n",
       "       1.27259205e-06, 9.99994393e-01, 9.99999995e-01, 1.48427648e-06,\n",
       "       2.13537704e-05, 6.09230192e-04, 8.41446299e-07, 1.96902698e-03,\n",
       "       9.99895472e-01, 9.99999985e-01, 6.38540120e-06, 3.10604254e-07,\n",
       "       9.99982380e-01, 6.06179803e-04, 3.90152927e-07, 9.99965062e-01,\n",
       "       2.91490388e-08, 6.08389170e-04, 9.97033519e-01, 3.42512091e-06,\n",
       "       9.99999851e-01, 9.96446454e-01, 3.92689984e-05, 9.99861657e-01,\n",
       "       3.58443301e-03, 2.11253259e-03, 4.49973438e-07, 9.99997930e-01,\n",
       "       1.77197491e-08, 2.81160787e-09, 3.48946847e-05, 9.99998785e-01,\n",
       "       9.98030602e-01, 9.99999861e-01, 5.06216497e-09, 4.53664116e-07,\n",
       "       9.99999422e-01, 9.99999861e-01, 9.99987149e-01, 1.25233604e-04,\n",
       "       9.99996863e-01, 9.99999961e-01, 1.29589223e-05, 6.10356894e-02,\n",
       "       5.98061318e-09, 1.22289720e-05, 1.20900319e-03, 1.85183612e-02,\n",
       "       1.49327222e-09, 3.39409591e-08, 1.22630592e-08, 3.48772869e-01,\n",
       "       4.02233899e-10, 5.50354499e-07, 1.28443161e-07, 9.99916393e-01,\n",
       "       9.99990592e-01, 6.86699816e-07, 7.43667436e-09, 7.87259996e-01,\n",
       "       9.99937038e-01, 9.99359569e-01, 1.12994863e-05, 9.99059867e-01,\n",
       "       1.78901056e-08, 1.51525238e-05, 9.99999991e-01, 9.99970807e-01,\n",
       "       7.35361281e-03, 9.99999998e-01, 9.99999973e-01, 4.55135800e-05,\n",
       "       9.99998023e-01, 9.99999976e-01, 9.99966924e-01, 9.99751239e-01,\n",
       "       9.99166967e-01, 9.72550384e-01, 3.46615821e-08, 5.28731658e-04,\n",
       "       2.87687486e-05, 2.42479159e-04, 9.99993110e-01, 7.89840199e-12,\n",
       "       1.38146167e-07, 8.25718717e-04, 9.99995759e-01, 6.87584871e-07,\n",
       "       1.86966940e-06, 6.27066138e-10, 9.97300433e-01, 9.16616881e-04,\n",
       "       9.99938856e-01, 3.04407343e-04, 1.45010483e-08, 9.41550585e-01,\n",
       "       9.99999364e-01, 7.15635248e-03, 9.99999936e-01, 1.41857282e-06,\n",
       "       6.36522265e-05, 9.99923044e-01, 9.99991555e-01, 9.99999733e-01,\n",
       "       9.98999472e-01, 3.28863781e-04, 9.99997493e-01, 1.12908245e-04,\n",
       "       9.99848197e-01, 2.94702420e-04, 9.99986410e-01, 9.99801565e-01,\n",
       "       7.31130271e-08, 1.03112959e-05, 8.75311653e-01, 9.99989785e-01,\n",
       "       9.99998078e-01, 8.75049892e-10, 1.00000000e+00, 1.87128834e-05,\n",
       "       2.12233608e-09, 5.72835430e-08, 1.58860215e-06, 9.99389734e-01,\n",
       "       6.24134432e-06, 9.39270653e-06, 6.18173869e-05, 9.35444128e-10,\n",
       "       1.95948200e-08, 9.99997376e-01, 4.89468281e-07, 2.90041790e-06,\n",
       "       9.91854742e-01, 9.95866807e-01, 9.99999895e-01, 3.95201478e-06,\n",
       "       9.08776597e-09, 9.99995829e-01, 3.23898034e-02, 5.24819867e-06,\n",
       "       9.99999996e-01, 1.00000000e+00, 9.99999992e-01, 1.99160829e-03,\n",
       "       8.65586994e-10, 6.25415343e-06, 1.80657102e-08, 2.78697327e-06,\n",
       "       9.99999439e-01, 2.11737640e-04, 9.99999766e-01, 9.65270058e-06,\n",
       "       1.18792987e-03, 9.99994008e-01, 7.53870851e-03, 9.99997980e-01,\n",
       "       9.99999832e-01, 9.99145438e-01, 9.99997582e-01, 9.98758181e-01,\n",
       "       8.50052505e-09, 9.99992212e-01, 9.99998099e-01, 1.00000000e+00,\n",
       "       1.84743017e-03, 8.05683794e-04, 9.99987184e-01, 5.28809476e-10,\n",
       "       9.99821063e-01, 3.31306876e-06, 4.79725567e-07, 1.04849048e-06,\n",
       "       5.56663217e-06, 1.40971781e-05, 3.91433310e-07, 3.18823801e-08,\n",
       "       2.23984388e-04, 9.99999204e-01, 9.99998325e-01, 1.59465775e-06,\n",
       "       9.99983693e-01, 9.99997431e-01, 9.99917126e-01, 9.99989123e-01,\n",
       "       1.27442347e-09, 2.63288977e-07, 1.59341940e-08, 9.97688886e-01,\n",
       "       1.13083728e-07, 1.14004231e-06, 1.13479510e-06, 9.99980235e-01,\n",
       "       9.99999574e-01, 6.93748139e-06, 1.27987727e-09, 5.73034760e-07,\n",
       "       1.01789244e-07, 9.99099945e-01, 1.14867382e-05, 1.26016480e-05,\n",
       "       5.55854868e-07, 1.22580734e-06, 9.99998844e-01, 5.19658780e-11,\n",
       "       3.68115051e-10, 9.99999863e-01, 9.99998354e-01, 1.93788093e-03,\n",
       "       9.99998189e-01, 6.12122576e-06, 1.33151641e-07, 9.99999924e-01,\n",
       "       5.38883683e-04, 9.99997257e-01, 9.99999992e-01, 9.99980198e-01,\n",
       "       9.99985036e-01, 9.99989993e-01, 1.16414393e-06, 3.99656347e-07,\n",
       "       9.99897995e-01, 9.99999975e-01, 2.62779694e-05, 9.99999932e-01,\n",
       "       9.99885569e-01, 4.39607477e-08, 9.99999331e-01, 9.85744900e-01,\n",
       "       5.03625416e-07, 9.99999596e-01, 9.41741831e-06, 8.16868273e-06,\n",
       "       9.99915245e-01, 4.43203196e-09, 9.99999405e-01, 1.77448351e-05,\n",
       "       9.99685048e-01, 3.66130421e-12, 9.99999987e-01, 1.31891435e-06,\n",
       "       9.72364498e-09, 8.32312617e-08, 1.00000000e+00, 2.05310154e-07,\n",
       "       9.99193461e-01, 2.67621762e-06, 3.65598032e-06, 1.72376362e-09,\n",
       "       7.37880134e-08, 9.99999939e-01, 1.54794618e-10, 2.90697498e-02,\n",
       "       1.23290411e-05, 9.99065510e-01, 1.45382454e-07, 2.07724243e-07,\n",
       "       3.33174674e-02, 1.09287723e-06, 2.00604762e-11, 9.99993776e-01,\n",
       "       9.99991311e-01, 1.62002158e-06, 3.92240444e-04, 5.01307785e-06,\n",
       "       9.99999797e-01, 6.89332722e-06, 7.28140717e-05, 9.99579468e-01,\n",
       "       9.99971409e-01, 1.01133264e-05, 5.39523070e-04, 9.99989673e-01,\n",
       "       9.99984460e-01, 5.31752387e-05, 5.13808546e-08, 6.00374263e-09,\n",
       "       5.36198246e-04, 5.42627813e-05, 9.99952615e-01, 2.92062811e-04,\n",
       "       9.99995443e-01, 4.34106454e-03, 4.45790203e-08, 5.07828042e-06,\n",
       "       9.99999979e-01, 6.65476644e-04, 9.99999920e-01, 2.48947611e-06,\n",
       "       9.99984184e-01, 9.99999958e-01, 9.99999981e-01, 1.09693956e-08,\n",
       "       1.98298264e-06, 6.05137803e-05, 9.89083791e-01, 6.29507996e-09,\n",
       "       5.02981186e-06, 2.66698154e-05, 9.99998107e-01, 4.36131450e-08,\n",
       "       3.81273527e-09, 9.63579183e-10, 9.48814807e-01, 9.99999996e-01,\n",
       "       9.99999565e-01, 7.44687519e-08, 1.76376586e-06, 9.99788168e-01,\n",
       "       4.23854286e-03, 1.85481508e-02, 2.94663746e-09, 7.34770701e-07,\n",
       "       9.99973619e-01, 2.25264240e-06, 6.78546441e-01, 9.99999991e-01,\n",
       "       1.05284801e-05, 6.46663770e-08, 3.68447462e-08, 6.73320826e-04,\n",
       "       6.36709752e-10, 1.24205981e-02, 9.86393811e-01, 4.71730059e-04,\n",
       "       9.99996827e-01, 5.64490344e-05, 1.36056608e-07, 9.99907668e-01,\n",
       "       7.90135010e-03, 7.31427306e-07, 9.99838113e-01, 9.99627955e-01,\n",
       "       1.89568810e-03, 5.07758575e-05, 6.85140298e-07, 2.01469877e-07,\n",
       "       1.19327506e-01, 1.43163965e-03, 6.19592419e-05, 5.60411715e-06,\n",
       "       4.00942208e-05, 9.99999415e-01, 3.35549368e-02, 2.38552068e-08,\n",
       "       9.99996951e-01, 5.27154821e-06, 1.11683200e-02, 2.57022288e-05,\n",
       "       9.99999916e-01, 4.35332525e-03, 3.30258322e-06, 2.46450165e-04,\n",
       "       4.86544757e-06, 1.94689616e-04, 7.44070322e-09, 1.08967520e-03,\n",
       "       1.00000000e+00, 9.89944033e-01, 8.95077965e-09, 9.99874497e-01,\n",
       "       9.99999999e-01, 9.99986097e-01, 7.75388185e-05, 9.99212464e-01,\n",
       "       9.99834886e-01, 4.07745125e-04, 9.99999982e-01, 9.97662922e-01,\n",
       "       9.99810561e-01, 9.99967748e-01, 1.82527392e-10, 9.99999995e-01,\n",
       "       9.99422176e-01, 1.49412429e-08, 9.99999823e-01, 9.99995104e-01,\n",
       "       9.99999998e-01, 2.95678285e-06, 9.99999976e-01, 5.34561049e-08,\n",
       "       9.99933337e-01, 9.99999968e-01, 5.42245202e-06, 9.91597861e-01,\n",
       "       3.93299050e-09, 8.72233632e-08, 2.29896050e-07, 8.50475726e-06,\n",
       "       9.99992472e-01, 9.99999949e-01, 9.99890402e-01, 6.88760364e-04,\n",
       "       2.78011302e-09, 9.99997628e-01, 4.65254822e-06, 9.99999999e-01,\n",
       "       9.99997779e-01, 4.84245930e-05, 9.99018878e-01, 6.62636525e-08,\n",
       "       1.62136034e-03, 9.34866351e-08, 1.81772097e-08, 9.99999994e-01,\n",
       "       9.92326616e-01, 9.99355268e-01, 4.89121880e-07, 1.50203691e-05,\n",
       "       9.99999977e-01, 2.35005565e-03, 9.99952772e-01, 9.99991129e-01,\n",
       "       1.35162691e-10, 1.62289648e-09, 1.53894171e-08, 1.50354829e-05,\n",
       "       1.84582093e-04, 4.75854335e-06, 1.17277050e-06, 4.91370700e-08,\n",
       "       1.94230420e-05, 9.99999939e-01, 9.91106468e-01, 3.87848827e-07,\n",
       "       2.34573889e-04, 9.99951300e-01, 9.71505772e-01, 1.26535413e-07,\n",
       "       1.26767403e-08, 1.05605244e-02, 5.33890850e-08, 5.68944858e-06,\n",
       "       7.40366281e-07, 7.57293499e-06, 9.99744370e-01, 4.69238257e-08,\n",
       "       4.73549337e-07, 9.98814649e-01, 7.27720047e-05, 9.90127445e-07,\n",
       "       9.98845676e-01, 3.24728489e-06, 6.64392377e-02, 9.99997893e-01,\n",
       "       1.25466198e-07, 9.99999988e-01, 9.99970839e-01, 2.98460907e-04,\n",
       "       2.89831569e-05, 9.99995500e-01, 9.99999998e-01, 9.94445224e-01,\n",
       "       4.94646908e-05, 1.12656388e-07, 1.17969001e-03, 5.36844169e-06,\n",
       "       1.60075159e-06, 9.99999827e-01, 9.92585518e-01, 1.54683495e-05,\n",
       "       2.70494532e-07, 9.99997396e-01, 9.91609193e-01, 9.75736945e-01,\n",
       "       9.99969082e-01, 9.99908952e-01, 9.99967116e-01, 5.50433199e-09,\n",
       "       5.00202125e-06, 6.12371090e-06, 1.52047635e-08, 9.99995713e-01,\n",
       "       5.48795880e-01, 9.99999895e-01, 7.34347196e-05, 9.99998353e-01,\n",
       "       9.99725582e-01, 2.03612401e-05, 9.99684398e-01, 9.99999068e-01,\n",
       "       7.67988757e-08, 9.99594238e-01, 9.99999381e-01, 1.52580732e-10,\n",
       "       7.66171531e-06, 5.01529552e-03, 2.31191629e-07, 1.23050899e-02,\n",
       "       7.39113417e-06, 1.16813759e-07, 2.90060515e-05, 5.75875448e-05,\n",
       "       4.55790438e-08, 1.12324709e-07, 9.99997482e-01, 9.99999987e-01,\n",
       "       4.84611151e-05, 9.99924623e-01, 2.76421531e-05, 9.99353688e-01,\n",
       "       9.99999215e-01, 9.99864380e-01, 9.48088827e-08, 1.54973890e-04,\n",
       "       4.53288789e-08, 3.00744144e-04, 9.31463942e-03, 9.99827413e-01,\n",
       "       1.00000000e+00, 9.99990372e-01, 9.99999984e-01, 5.81369311e-07,\n",
       "       9.98621018e-01, 9.99981911e-01, 1.77246213e-04, 3.39654181e-02,\n",
       "       9.99905694e-01, 5.31678401e-07, 9.99999997e-01, 1.67109922e-04,\n",
       "       9.99999984e-01, 8.73977504e-08, 1.09339444e-05, 9.99999971e-01,\n",
       "       1.73208819e-04, 2.10057816e-09, 2.34330004e-08, 4.26837815e-04,\n",
       "       9.99282272e-01, 9.99947737e-01, 1.45550404e-06, 1.38309220e-06,\n",
       "       3.67605863e-08, 9.99997923e-01, 1.50074664e-06, 2.56277259e-02,\n",
       "       2.59959306e-06, 9.97840778e-01, 9.67217557e-07, 9.99766544e-01,\n",
       "       2.61539874e-03, 1.78291659e-02, 9.97218025e-01, 9.99926529e-01,\n",
       "       1.21561290e-04, 9.99999995e-01, 9.95637390e-01, 9.99987672e-01,\n",
       "       9.99885904e-01, 4.37335897e-04, 5.33599497e-07, 9.99999923e-01,\n",
       "       3.18275494e-08, 7.05899947e-05, 9.99999678e-01, 1.25635102e-04,\n",
       "       3.52540806e-09, 4.86268953e-06, 2.65923626e-05, 9.99999993e-01,\n",
       "       9.63299927e-01, 9.99996913e-01, 9.95377603e-01, 3.13735908e-01,\n",
       "       9.99972202e-01, 4.70506657e-02, 9.99928304e-01, 9.99983840e-01,\n",
       "       5.75144426e-05, 1.14926715e-03, 9.99981370e-01, 1.25699802e-04,\n",
       "       2.75115136e-11, 3.64737603e-02, 3.55761203e-08, 9.99999997e-01,\n",
       "       1.34003111e-06, 1.28080825e-08, 3.15268786e-03, 7.61231757e-03,\n",
       "       5.39729354e-03, 9.13552380e-08, 9.99998629e-01, 2.14961598e-08,\n",
       "       3.80253883e-06, 9.86408058e-06, 4.48949642e-06, 9.99999123e-01,\n",
       "       6.55906742e-03, 9.98156339e-01, 9.50019864e-07, 9.47098349e-01,\n",
       "       1.55365806e-08, 1.40929618e-04, 9.99999271e-01, 2.61627068e-05,\n",
       "       9.99993728e-01, 9.99829377e-01, 9.99994618e-01, 7.61955632e-05,\n",
       "       9.93907641e-01, 6.55322726e-06, 9.99999799e-01, 3.37135786e-07,\n",
       "       2.48448627e-10, 4.39271929e-06, 9.99999828e-01, 8.80742988e-09,\n",
       "       8.55637611e-06, 1.00000000e+00, 1.90551436e-02, 1.20733998e-07,\n",
       "       1.22052718e-04, 1.46846481e-04, 2.76487766e-10, 9.99993087e-01,\n",
       "       1.22169501e-07, 2.71149353e-06, 1.50456330e-03, 9.99794928e-01,\n",
       "       7.08385643e-08, 2.47412778e-06, 9.99848151e-01, 3.92891291e-06,\n",
       "       3.64037254e-08, 9.99999923e-01, 9.80437916e-07, 7.30789459e-07,\n",
       "       1.17015112e-07, 8.88325458e-06, 9.99545820e-01, 9.30105437e-06,\n",
       "       3.78319704e-07, 9.99998763e-01, 4.53245524e-07, 9.78274936e-04,\n",
       "       1.00000000e+00, 1.37830721e-03, 9.99999724e-01, 4.70216595e-07,\n",
       "       1.25655149e-05, 9.99999987e-01, 9.99894854e-01, 6.30549456e-06,\n",
       "       7.67769555e-05, 4.06884861e-06, 2.83570181e-02, 9.94574185e-01,\n",
       "       9.99996958e-01, 7.70354490e-04, 1.00000000e+00, 7.66625985e-07,\n",
       "       9.99732105e-01, 4.52252501e-07, 9.99999092e-01, 9.99999826e-01,\n",
       "       9.99990437e-01, 3.53381855e-03, 9.99378643e-01, 4.42098556e-08,\n",
       "       9.99999958e-01, 9.99998994e-01, 5.42779698e-03, 9.99999983e-01,\n",
       "       4.78477753e-04, 9.99991567e-01, 9.99999997e-01, 8.37799854e-06,\n",
       "       1.40666671e-07, 1.77789291e-08, 1.00339823e-09, 1.73531663e-02,\n",
       "       3.03250231e-02, 9.99999999e-01, 4.52918067e-05, 2.45047122e-04,\n",
       "       3.55711214e-05, 9.99999935e-01, 1.32513251e-01, 6.48990888e-08,\n",
       "       1.23734219e-05, 9.99999941e-01, 2.28849301e-08, 5.87181451e-02,\n",
       "       1.65624888e-06, 2.93664191e-05, 2.81850121e-06, 9.99916200e-01,\n",
       "       1.29252524e-05, 9.99997751e-01, 1.39529366e-04, 5.80484615e-11,\n",
       "       6.94356420e-06, 2.00107522e-05, 8.10199172e-06, 9.99998572e-01,\n",
       "       4.44952784e-06, 5.56500871e-04, 7.98261859e-05, 9.99998664e-01,\n",
       "       1.00760997e-03, 7.03265645e-09, 9.99996434e-01, 1.97325924e-02,\n",
       "       2.49902357e-09, 9.99997510e-01, 2.08004408e-05, 5.11665087e-07,\n",
       "       9.99999533e-01, 4.79513620e-06, 4.27595806e-06, 6.82398908e-07,\n",
       "       7.71108612e-05, 9.99999028e-01, 3.00535818e-07, 9.99999227e-01,\n",
       "       4.20292801e-05, 1.11794606e-04, 3.15108355e-08, 1.51111138e-06,\n",
       "       1.54572995e-04, 3.90825881e-04, 9.99972898e-01, 1.19429763e-06,\n",
       "       9.99977928e-01, 4.86862835e-06, 1.09881909e-07, 9.99866558e-01,\n",
       "       7.88017033e-01, 9.99975052e-01, 4.35830752e-06, 3.07345423e-03,\n",
       "       1.50917636e-06, 6.70183798e-09, 7.32185437e-06, 2.84778069e-05,\n",
       "       9.99171128e-01, 1.29475653e-06, 1.94203445e-06, 2.98524993e-08,\n",
       "       5.43636636e-06, 9.99997720e-01, 9.99896076e-01, 2.99177712e-04,\n",
       "       2.00868360e-08, 6.08472881e-06, 9.99897905e-01, 9.99988350e-01,\n",
       "       9.99987155e-01, 9.23397200e-10, 9.95258784e-01, 9.98772913e-01,\n",
       "       5.47891422e-06, 2.64399356e-09, 9.80647904e-09, 8.71590722e-08,\n",
       "       1.00000000e+00, 1.00000000e+00, 4.99389036e-04, 1.87308892e-06,\n",
       "       9.99999704e-01, 9.99615740e-01, 9.99999455e-01, 2.91748470e-08,\n",
       "       9.99999922e-01, 4.96665376e-07, 9.99858123e-01, 4.26236124e-07,\n",
       "       9.99999997e-01, 2.74240849e-04, 9.99959719e-01, 6.86607170e-05,\n",
       "       9.99999994e-01, 2.62129999e-06, 1.06255787e-08, 9.99998366e-01,\n",
       "       2.00802913e-05, 9.99803048e-01, 9.99998624e-01, 2.48046978e-06,\n",
       "       9.99994335e-01, 3.02947383e-05, 3.96317681e-05, 9.94467292e-01,\n",
       "       9.99995252e-01, 2.92325515e-09, 1.01597828e-06, 9.99999998e-01,\n",
       "       3.57974641e-04, 5.62436533e-06, 9.99999210e-01, 9.99999944e-01,\n",
       "       1.15694207e-06, 6.11582018e-08, 9.99998564e-01, 1.20803084e-07,\n",
       "       3.92601368e-03, 1.40315162e-03, 9.99990206e-01, 4.91532168e-04,\n",
       "       1.01105331e-05, 2.93693170e-01, 2.65320230e-06, 4.60632035e-08,\n",
       "       1.21361563e-04, 5.29290865e-04, 9.99999975e-01, 6.46975054e-05,\n",
       "       9.99992206e-01, 3.31518813e-04, 1.18579980e-03, 2.11879463e-07,\n",
       "       7.17657002e-05, 9.33554184e-07, 7.82900415e-04, 2.35452166e-09,\n",
       "       9.99883303e-01, 1.45544879e-03, 2.38848596e-05, 9.99752586e-01,\n",
       "       1.29660740e-05, 1.65768448e-04, 1.30616359e-06, 1.56178697e-06,\n",
       "       9.99994434e-01, 1.16319191e-05, 9.98917431e-01, 9.99987498e-01,\n",
       "       9.99999979e-01, 1.41544274e-02, 6.15956300e-10, 1.46340539e-05,\n",
       "       3.62194876e-02, 2.05481601e-04, 2.89209377e-06, 1.15944871e-05,\n",
       "       9.94269288e-01, 1.49332349e-04, 1.05479677e-04, 9.99999979e-01,\n",
       "       8.02436331e-07, 9.99999839e-01, 1.19263980e-06, 1.00000000e+00,\n",
       "       1.42551001e-07, 5.50008386e-04, 9.99975405e-01, 1.00000000e+00,\n",
       "       9.99936542e-01, 6.06571194e-03, 1.40903122e-06, 1.64276065e-07,\n",
       "       3.84105786e-11, 2.34832413e-09, 1.26577512e-08, 2.28570897e-05,\n",
       "       7.02582120e-07, 3.39969527e-09, 2.94421477e-11, 1.00159207e-09,\n",
       "       7.29897690e-06, 9.99999999e-01, 9.99999963e-01, 9.99751616e-01,\n",
       "       3.71003033e-05, 1.00000000e+00, 9.90904625e-01, 9.99999991e-01,\n",
       "       1.11677633e-05, 9.99992471e-01, 9.99997653e-01, 9.99999905e-01,\n",
       "       1.04062308e-07, 3.90908724e-04, 9.99999782e-01, 9.99999993e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Estep(x, mu, sigma, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the estep is indeed our classification probability, if we want to use our unsupervized model to make a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "zpred = 1*(Estep(x, mu, sigma, lam) >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(z==zpred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival Analysis using EM\n",
    "\n",
    "The critical problem in survival analysis is that data tends to be right-censored. Imaginw a medical study following a cohort which has got cancer treatment. The study ends 5 years from start. Upto then, some patients have died. We record the time they died. But some were alive past the 5 years, and we stop recording, so we dont know, how long further from 5 years they will continue to live.\n",
    "\n",
    "There are some key concepts and definitions we need.\n",
    "\n",
    "Define T as a random lifetime taken from the population.\n",
    "\n",
    "The **survival function** $S(t)$ of a population is defined as:\n",
    "\n",
    "$$S(t) = P(T > t), $$\n",
    "\n",
    "the probability of surviving past time t. The function $S$ is 1 minus the CDF of T, or $$S(t) = 1 - CDF_T(t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Hazard Function** is defined as the probability that the death event occurs at time t, conditional on the fact that the death event has not occurred until time t. \n",
    "\n",
    "In other words, we care about:\n",
    "\n",
    "$$\\lambda(t) =  \\lim_{\\delta t \\rightarrow 0 } \\; \\frac{P( t \\le T \\le t + \\delta t \\mid T > t)}{\\delta t}$$\n",
    "\n",
    "Using Bayes Theorem we can see that:\n",
    "\n",
    "$$\\lambda(t) = \\frac{-S'(t)}{S(t)}$$\n",
    "\n",
    "Solving this differential equation needs:\n",
    "\n",
    "$$S(t) = \\exp\\left( -\\int_0^t \\lambda(z) \\mathrm{d}z \\right)$$\n",
    "\n",
    "For a constant Hazard Function, this is just an exponential model:\n",
    "\n",
    "$$f_T(t) = \\lambda(t)\\exp\\left( -\\int_0^t \\lambda(z) \\mathrm{d}z \\right)$$\n",
    "\n",
    "(Formulae are taken from the docs for Python's lifelines package)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think we are being too morbid here, rest assured that survival analysis finds applications in many places, survival of politicians in office, survival os machinery and circuits. For example, a Weibull distribution which is concave is used to model all of early failures, random failures, and age related wear and tear, while the exponential distribution is used when u dont know how old the parts are (as then any memory based distribution is out). \n",
    "\n",
    "In general, you might even assume a non-parametric hazard and use techniques such as Kaplan-Meier estimation and Cox's proportional hazards (for regressions on subject covariates).\n",
    "\n",
    "But let us focus on the exponential distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, when our study ends, some patients are still alive. So we have incomplete data $z_m$ as the unseen lifetime for those censored, and the data we observe:\n",
    "\n",
    "$$x_m = (a_m, d_m)$$\n",
    "\n",
    "where $a_m$ is the age reached, and $d_m =1$ if the data was not censored and $d_m = 0$ if it was.\n",
    "\n",
    "We assume that the age of death is exponentially distributed with constant hazard $\\lambda$, so that\n",
    "\n",
    "$$\\ell(\\{x_m\\}, \\{z_m\\} \\mid \\lambda) = log \\left( \\prod_{m=1}^{n} \\frac{1}{\\lambda} e^{-\\lambda x_m} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data up into censored and non-censored data, and expanding the log-likelihood we get:\n",
    "\n",
    "$$\\ell(\\{x_m\\}, \\{z_m\\} \\mid \\lambda) = -n log \\lambda - \\frac{1}{\\lambda} \\sum_{m=1}^{n} x_m = -n log \\lambda - \\frac{1}{\\lambda} \\sum_{m=1}^{n}  \\left( a_m d_m + (1 - d_m) z_m \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the $Q$ function is the z-posterior expectation of the full-data likelihood, so that:\n",
    "\n",
    "$$Q(\\lambda, \\lambda_{old}) = E_z \\left[\\ell(\\{x_m\\}, \\{z_m\\} \\mid \\lambda) \\mid \\lambda_{old} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we only need the expectations of the $z$ with respect to their posterior. Since we assume exponential distribution, we are assuming that our data is \"memoryless\". That is, given that we have survived till censoring, or expected survival time beyond that is simply $\\lambda_{old}$, giving a total expected age equal to censoring-time plus $\\lambda_{old}$.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$Q(\\lambda, \\lambda_{old}) = -n log \\lambda - \\frac{1}{\\lambda}  \\sum_{m=1}^{n} \\left( a_m d_m + (1 - d_m) (c_m + \\lambda_{old}) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does the e-step. The m-step simply differentiates this with respect to lambda to find the x-data MLE.\n",
    "\n",
    "$$\\lambda_{MLE} = \\frac{1}{n} \\left[\\sum_{m=1}^{n} \\left( a_m d_m + (1 - d_m) (c_m + \\lambda_{old}) \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(lamo, data):\n",
    "    n = data.shape[0]\n",
    "    summer = np.sum(data.a*data.d + (1-data.d)*(data.c + lamo))\n",
    "    return summer/n\n",
    "def run_em(data, lam_init, n_iter):\n",
    "    lamo = lam_init\n",
    "    for i in range(n_iter):\n",
    "        lam_mle = iteration(lamo, data)\n",
    "        lamo = lam_mle\n",
    "        print(\"iteration i\", i, lamo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hers is the data description:\n",
    "\n",
    "```\n",
    "Head and neck cancer from the Northeren California Oncology Group (NCOG)\n",
    "Data matrix gives detailed time information, with t (months till death/censoring) being used in our analysis.\n",
    "The column d is the death(1)/censoring(0) indicator, and arm the treatment arm (A Chemotherapy, B Chemotherapy + Radiation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>t</th>\n",
       "      <th>d</th>\n",
       "      <th>arm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>78</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>78</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>78</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>78</td>\n",
       "      <td>277</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>78</td>\n",
       "      <td>440</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  month  year    t  d arm\n",
       "0   19      6    78  248  1   A\n",
       "1   15     12    78  160  1   A\n",
       "2   20      7    78  319  0   A\n",
       "3   17     11    78  277  1   A\n",
       "4   15     12    78  440  1   A"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdata = pd.read_csv(\"data/ncog.txt\", sep=\" \")\n",
    "sdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>t</th>\n",
       "      <th>d</th>\n",
       "      <th>arm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>78</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>80</td>\n",
       "      <td>1226</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>81</td>\n",
       "      <td>1116</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>1412</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>81</td>\n",
       "      <td>1349</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>81</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>83</td>\n",
       "      <td>523</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>279</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>78</td>\n",
       "      <td>2146</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>2297</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>79</td>\n",
       "      <td>528</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>2023</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>1771</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>1897</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>80</td>\n",
       "      <td>1642</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "      <td>1245</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>81</td>\n",
       "      <td>1331</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>1092</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>759</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>83</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>83</td>\n",
       "      <td>547</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    day  month  year     t  d arm\n",
       "2    20      7    78   319  0   A\n",
       "21    6     11    80  1226  0   A\n",
       "24   22      1    81    74  0   A\n",
       "27   12      6    81  1116  0   A\n",
       "28    6      8    81  1412  0   A\n",
       "30   15      7    81  1349  0   A\n",
       "31   23     10    81   185  0   A\n",
       "46   23     11    83   523  0   A\n",
       "50    6      7    84   279  0   A\n",
       "54    4     12    78  2146  0   B\n",
       "55    5      3    79  2297  0   B\n",
       "56   12      3    79   528  0   B\n",
       "59   16      4    79   169  0   B\n",
       "63   13      9    79  2023  0   B\n",
       "67   27      3    80  1771  0   B\n",
       "68   14      2    80  1897  0   B\n",
       "74   19     12    80  1642  0   B\n",
       "79   18      9    81  1245  0   B\n",
       "81   29     10    81  1331  0   B\n",
       "82    5      3    82  1092  0   B\n",
       "89   20      5    83   759  0   B\n",
       "92    6     10    83   613  0   B\n",
       "93   23     11    83   547  0   B"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdata[sdata.d==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata['a'] = sdata['c'] = sdata.t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>t</th>\n",
       "      <th>d</th>\n",
       "      <th>arm</th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>78</td>\n",
       "      <td>248</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>248</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>78</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>78</td>\n",
       "      <td>319</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>78</td>\n",
       "      <td>277</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>277</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>78</td>\n",
       "      <td>440</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>440</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  month  year    t  d arm    a    c\n",
       "0   19      6    78  248  1   A  248  248\n",
       "1   15     12    78  160  1   A  160  160\n",
       "2   20      7    78  319  0   A  319  319\n",
       "3   17     11    78  277  1   A  277  277\n",
       "4   15     12    78  440  1   A  440  440"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357.84313725490193, 639.2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_avg = np.mean(sdata[sdata.arm=='A'].t)\n",
    "B_avg = np.mean(sdata[sdata.arm=='B'].t)\n",
    "A_avg, B_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration i 0 420.991926182\n",
      "iteration i 1 432.135830111\n",
      "iteration i 2 434.102401392\n",
      "iteration i 3 434.449443383\n",
      "iteration i 4 434.510686087\n",
      "iteration i 5 434.521493623\n",
      "iteration i 6 434.523400835\n",
      "iteration i 7 434.523737402\n",
      "iteration i 8 434.523796796\n",
      "iteration i 9 434.523807278\n"
     ]
    }
   ],
   "source": [
    "run_em(sdata[sdata.arm=='A'], A_avg, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration i 0 838.062222222\n",
      "iteration i 1 899.930469136\n",
      "iteration i 2 919.178368176\n",
      "iteration i 3 925.166603432\n",
      "iteration i 4 927.029609957\n",
      "iteration i 5 927.609211987\n",
      "iteration i 6 927.789532618\n",
      "iteration i 7 927.84563237\n",
      "iteration i 8 927.863085626\n",
      "iteration i 9 927.868515528\n",
      "iteration i 10 927.870204831\n",
      "iteration i 11 927.870730392\n",
      "iteration i 12 927.8708939\n",
      "iteration i 13 927.870944769\n",
      "iteration i 14 927.870960595\n"
     ]
    }
   ],
   "source": [
    "run_em(sdata[sdata.arm=='B'], B_avg, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other resons why is EM important.\n",
    "\n",
    "We have motivated the EM algorithm using mixture models and missing data, but that is not its only place of use. \n",
    "\n",
    "Since MLE's can overfit, we often prefer to use MAP estimation. EM is a perfectly reasonable method for MAP estimation in mixture models; you just need to multiply in the prior.\n",
    "\n",
    "Basically the EM algorithm has a similar setup to the data augmentation problem and can be used in any problem which has a similar structure. Suppose for example you have two parameters $\\phi$ and $\\gamma$ in a posterior estimation, with daya $y$. Say that we'd like to estimate the posterior $p(\\phi  \\vert  y)$. It may be relatively hard to estimate this, but suppose we can  work with $p(\\phi  \\vert  \\gamma, y)$ and $p(\\gamma  \\vert  \\phi, y)$. Then you can use the structure of the EM algorithm to estimate the marginal posterior of any one parameter. Start with:\n",
    "\n",
    "$$log p(\\phi  \\vert  y) = log p(\\gamma, \\phi  \\vert  y) - log p(\\gamma  \\vert  \\phi, y)$$\n",
    "\n",
    "Notice the similarity of this to the above expressions with $\\phi$ as $x$, $y$ as $\\theta$, and $\\gamma$ as $z$. Thus the same derivations apply toany problem with this structure.\n",
    "\n",
    "This structure can also be used in type-2 likelihood or emprical bayes estimation."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
